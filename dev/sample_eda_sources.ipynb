{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c6b5b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22864/1687165248.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# replace with full dataset if desired\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# replace with full dataset if desired\n",
    "df = pd.read_csv('sample_data.csv')\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206667b",
   "metadata": {},
   "source": [
    "## NOTES FROM PARTER MEETING 9/10/22\n",
    "________________________________________________________________\n",
    "#### use SpaCy to grab nouns/verbs, use their tagging functions to grab instances of people, geolocations, etc.\n",
    "#### after getting this data, create features/attributes based on the data and join to the individual sources. Cluster\n",
    "#### on datasets aggregated by sources and by country (and anything else we think of)\n",
    "\n",
    "#### cluster word vectors (maps to dimensions and takes euclidean or other distances)\n",
    "#### overall, try a lot of clustering\n",
    "\n",
    "#### if we want to dive even deeper into NLP nuances, prof uploaded NLP video on onedrive (use second link not first)\n",
    "\n",
    "#### characterizing the sources themselves by quality or anything else could be good idea\n",
    "#### , (whats a good source for information x), who reports factual, who uses adjectives, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['source', 'source_scale', 'notes']\n",
    "sources = df[cols]\n",
    "\n",
    "# some combinations of sources are in different orders. Creating source_cleaned to fix this\n",
    "sources['source_cleaned'] = [str(sorted(s.split('; '))) for s in sources['source']]\n",
    "\n",
    "sources.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(10, 10))\n",
    "palette = 'viridis'\n",
    "\n",
    "sns.countplot(ax=ax[0],palette=palette, y=sources['source_scale'], order=pd.value_counts(sources['source_scale']).index)\n",
    "sns.countplot(ax=ax[1],palette=palette, y=sources['source_scale'], order=pd.value_counts(sources['source_scale'])[:10].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b984d",
   "metadata": {},
   "source": [
    "#### It looks like the majority of entries are on a national scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db534873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since sources that appear to report the same event are separated by a semicolon, we have to expand this column to get the\n",
    "# count of times a source has reported on an event\n",
    "sources_list = list()\n",
    "for s in sources['source']:\n",
    "    for i in s.split('; '):\n",
    "        sources_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting into Pandas df to get counts\n",
    "source_expanded = pd.DataFrame({'source': sources_list})\n",
    "source_expanded.groupby('source')['source'].size().reset_index(name='count').sort_values('count', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04af4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(10, 10))\n",
    "palette = 'viridis'\n",
    "\n",
    "# Plot of sources\n",
    "sns.countplot(ax=ax[0], palette=palette, y=sources['source_cleaned'], order=pd.value_counts(sources['source_cleaned']).iloc[:20].index)\n",
    "\n",
    "# Plot of individually counted sources\n",
    "sns.countplot(ax=ax[1], palette=palette, y=source_expanded['source'], order=pd.value_counts(source_expanded['source']).iloc[:10].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a9627",
   "metadata": {},
   "source": [
    "### Yemen Data Project appears very common in the individually counted sources, but raw sources only show the source paired with another source.\n",
    "\n",
    "### Exploring these below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b904979",
   "metadata": {},
   "outputs": [],
   "source": [
    "yemen = sources[sources['source_cleaned'].str.contains('Yemen Data Project')]\n",
    "sns.countplot(y=yemen['source_cleaned'], order=pd.value_counts(yemen['source_cleaned']).iloc[:20].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59f70c",
   "metadata": {},
   "source": [
    "# NLP on 'notes' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd76f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "# tokenizer that removes punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# creating one big string of all of the values in the 'notes' column\n",
    "text = ''\n",
    "for n in sources['notes']:\n",
    "    text += n + ' '\n",
    "    \n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# lemmatizing words\n",
    "lem_words = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "# getting frequency distribution\n",
    "dist_lem = FreqDist(lem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c507faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting most common words\n",
    "top_common = dist_lem.most_common(15)\n",
    "pdser = pd.Series(dict(top_common))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "all_plot = sns.barplot(x=pdser.index, y=pdser.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a33ecd",
   "metadata": {},
   "source": [
    "#### Looks like there were some stopwords. Removing those and trying again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords and replotting\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_cleaned = [w for w in tokens if w.lower() not in stop_words]\n",
    "\n",
    "lem_words_cleaned = [lemmatizer.lemmatize(w) for w in tokens_cleaned]\n",
    "dist_lem_cleaned = FreqDist(lem_words_cleaned)\n",
    "\n",
    "# plotting most common words\n",
    "top_common = dist_lem_cleaned.most_common(15)\n",
    "pdser = pd.Series(dict(top_common))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "all_plot = sns.barplot(x=pdser.index, y=pdser.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud().generate_from_frequencies(dist_lem_cleaned)\n",
    "plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b71900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying nlp() wrapper to obtain SpaCy attributes\n",
    "mytext = nlp(text)\n",
    "\n",
    "# counting named entities\n",
    "labels = [x.label_ for x in mytext.ents]\n",
    "counts = Counter(labels)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c897e7",
   "metadata": {},
   "source": [
    "# Creating DataFrame of joined individual sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need a list of distinct sources\n",
    "sources_distinct = list(set(sources_list))\n",
    "sources_distinct_df = pd.DataFrame({'source_singular': sources_distinct})\n",
    "\n",
    "# since eventually we're using a LIKE clause for the join, we need to add percentage wildcards here because we can't in pandasql.\n",
    "sources_distinct_df['source_singular'] = sources_distinct_df['source_singular'].apply(lambda x: f'%{x}%')\n",
    "\n",
    "# Second, join this df with the sources df with the help of pandasql\n",
    "\n",
    "from pandasql import sqldf \n",
    "sql = lambda q: sqldf(q, globals())\n",
    "\n",
    "expanded_source_df = sql('''\n",
    "    SELECT * FROM df s\n",
    "    JOIN sources_distinct_df sd\n",
    "    ON s.source LIKE sd.source_singular\n",
    "''')\n",
    "\n",
    "# removing percentage wildcards\n",
    "expanded_source_df['source_singular'] = expanded_source_df['source_singular'].apply(lambda x: x.replace('%', ''))\n",
    "expanded_source_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8ec0d",
   "metadata": {},
   "source": [
    "#### Now, we can analyze singular sources based on other columns of interest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
