{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce3d13e",
   "metadata": {},
   "source": [
    "\n",
    "### geolocation taging\n",
    "\n",
    "We are creating tags for each source based on a majority percent of geolocations that the source is reporting on. We must also keep in mind how to generate these tags so that they benefit the userThere are three constraints the developers need to consider before generating the tags:\n",
    "\n",
    "- the minimum number of times the source has been used by ACLED\n",
    "- the maximum number of times the source has been used by ACLED\n",
    "- the minumum percentage of events from a country required to be a tag\n",
    "\n",
    "The tag should represent a source that contains an amount of data relating to the tagged geolocation that is managable for the user. We first considered how many events per source should be reporting on the geolocation in order to be tagged. For example, the source Twitter has been used by ACLED times, with % of them being in the United States. One may think that Twitter should be tagged as a good source of data for the US since it was used for 277 events. But if this source were tagged as such, and a user obtains this source in a search for \"COVID-19 civil unrest in the US\", they would also encounter 188 events not from the US. Sources used by ACLED a large number of times need to have a large enough percent from a specific country for it to be adequate for the user. However, setting a high percentage threshold for sources used by ACLED  a small amount of times may exclude sources with a managable amount of data not applicable to their needs. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e9db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# helper function to create a dataframe with each source and the number of times the source has been used by ACLED\n",
    "def _get_tot_cnt_df(df):\n",
    "    sql = lambda q: sqldf(q, locals())\n",
    "    \n",
    "    tot_cnt_df = sqldf(f'''\n",
    "        SELECT source_singular,\n",
    "        count(*) total\n",
    "        FROM df\n",
    "        GROUP BY source_singular\n",
    "    ''', locals())\n",
    "    \n",
    "    return tot_cnt_df\n",
    "    \n",
    "# helper function to modify column names to work with pandasql\n",
    "def _clean(c):\n",
    "    for r in [' ', '-', '/']:\n",
    "        c = c.replace(r, '_')\n",
    "    \n",
    "    for r in ['.', ',']:\n",
    "        c = c.replace(r, '')\n",
    "    \n",
    "    return c\n",
    "\n",
    "\n",
    "# filter dataframe to get tags for each source that meet the criteria to be a tag\n",
    "# params:\n",
    "#  tag_col: name of column to use as tag\n",
    "#  dst_df: df of distinct tagging values (required for SQL)\n",
    "#  type_pct_df: df of the percents of each tag in each source (required for SQL)\n",
    "#  total_min: the minimum number of times the source has been used by ACLED\n",
    "#  total_max: the maximum number of times the source has been used by ACLED\n",
    "#  pct_min: the minimum percent of events per source required to be from the tag value\n",
    "\n",
    "def _generate_tag_df(tag_col, dst_df, type_pct_df, total_min, total_max, pct_min):\n",
    "    # required for pandasql to work properly\n",
    "    type_pct_df_sql = type_pct_df\n",
    "    \n",
    "    # creating empty dataframe for each source, potential tag value, total times used with the potential tag value,\n",
    "    # and percentage of times used with the potential tag value\n",
    "    df = pd.DataFrame(columns=[\n",
    "        'source_singular',\n",
    "        tag_col,\n",
    "        f'{tag_col}_total',\n",
    "        f'{tag_col}_pct'\n",
    "    ])\n",
    "    \n",
    "    # iterating through every potential tag value and appending candidate tagged sources to the empty dataframe\n",
    "    for t in dst_df[tag_col]:\n",
    "        t_cln = _clean(t)\n",
    "        t_cln_pct_nm = t_cln + '_pct'\n",
    "\n",
    "        query = f'''\n",
    "        SELECT\n",
    "            source_singular,\n",
    "            \"{t_cln}_majority\" AS \"{tag_col}\",\n",
    "            {t_cln} AS \"{tag_col}_total\",\n",
    "            {t_cln_pct_nm} AS {tag_col}_pct\n",
    "        FROM\n",
    "            type_pct_df\n",
    "        WHERE \n",
    "            total >= {total_min}\n",
    "            AND total <= {total_max}\n",
    "            AND {t_cln_pct_nm} >= {pct_min}\n",
    "        '''\n",
    "\n",
    "        df1 = sqldf(query, locals())\n",
    "        df = pd.concat([df, df1])\n",
    "        \n",
    "    # returning dataframe of sources and candidate tags \n",
    "    return df\n",
    "\n",
    "\n",
    "# takes extended DF as parameter\n",
    "def tag_sub_event_type(df, total_min, total_max, pct_min):\n",
    "\n",
    "    # getting distinct list of specific event types from the \"sub_event_type\" column\n",
    "    typ_df = sqldf('''\n",
    "        SELECT DISTINCT sub_event_type\n",
    "        FROM df\n",
    "    ''', locals())\n",
    "\n",
    "    # generating query to get binary indicators for each sub_event_type\n",
    "    query_fmt = ''\n",
    "    for i, t in enumerate(typ_df['sub_event_type']):\n",
    "        t_cln = t.replace(\" \", \"_\")\n",
    "        t_cln = t_cln.replace(\"/\", \"_\")\n",
    "        query_fmt += f'sum(CASE WHEN sub_event_type = \"{t}\" THEN 1 ELSE 0 END ) AS \"{t_cln}\"'\n",
    "        if i < len(src_df['sub_event_type']) - 1:\n",
    "            query_fmt += ',\\n'\n",
    "\n",
    "    # applying above query\n",
    "    type_cnt_df = sqldf(f'''\n",
    "        SELECT source_singular,\n",
    "        {query_fmt}\n",
    "        FROM df\n",
    "        GROUP BY source_singular\n",
    "    ''', locals())\n",
    "\n",
    "    # getting dataframe of each source and the total of how many times the source was used by ACLED in an event\n",
    "    tot_cnt_df = _get_tot_cnt_df(df)\n",
    "    \n",
    "    # generating query to get percent of sub_event_type reported on by each source for each sub_event_type\n",
    "    query_fmt = ''\n",
    "    for i, t in enumerate(typ_df['sub_event_type']):\n",
    "        t_cln = t.replace(\" \", \"_\")\n",
    "        t_cln = t_cln.replace(\"/\", \"_\")\n",
    "        t_cln_pct_nm = t_cln + '_pct'\n",
    "        query_fmt += f'cast({t_cln} AS DOUBLE) / cast(total AS DOUBLE) \"{t_cln_pct_nm}\"'\n",
    "        if i < len(src_df['sub_event_type']) - 1:\n",
    "            query_fmt += ',\\n'\n",
    "\n",
    "    # applying above query\n",
    "    type_pct_df = sqldf(f'''\n",
    "        SELECT\n",
    "            a.*,\n",
    "            b.total,\n",
    "            {query_fmt}\n",
    "        FROM type_cnt_df a\n",
    "        JOIN tot_cnt_df b\n",
    "            ON a.source_singular = b.source_singular\n",
    "    ''', locals())\n",
    "\n",
    "    # filter dataframe to get sub_event_type for each source that meet the criteria to be a tag\n",
    "    sub_event_type_df = _generate_tag_df('sub_event_type', typ_df, type_pct_df, total_min, total_max, pct_min)\n",
    "    \n",
    "    return sub_event_type_df\n",
    "\n",
    "\n",
    "\n",
    "# takes extended DF as parameter\n",
    "def tag_country(df, total_min, total_max, pct_min):\n",
    "\n",
    "    # getting distinct list of geographical locations (countries) from the \"country\" column\n",
    "    geo_df = sqldf('''\n",
    "        SELECT DISTINCT country\n",
    "        FROM df\n",
    "    ''', locals())\n",
    "\n",
    "    # generating query to get binary indicators for each country\n",
    "    query_fmt = ''\n",
    "    for i, t in enumerate(geo_df['country']):\n",
    "        t_cln = _clean(t)\n",
    "        query_fmt += f'sum(CASE WHEN country = \"{t}\" THEN 1 ELSE 0 END ) AS \"{t_cln}\"'\n",
    "        if i < len(geo_df['country']) - 1:\n",
    "            query_fmt += ',\\n'\n",
    "\n",
    "    # applying above query\n",
    "    type_cnt_df = sqldf(f'''\n",
    "    SELECT source_singular,\n",
    "    {query_fmt}\n",
    "    FROM df\n",
    "    GROUP BY source_singular\n",
    "    ''', locals())\n",
    "    \n",
    "    # getting dataframe of each source and the total of how many times the source was used by ACLED in an event\n",
    "    tot_cnt_df = _get_tot_cnt_df(df)\n",
    "    \n",
    "    # generating query to get percent of countries reported on by each source for each country\n",
    "    query_fmt = ''\n",
    "    for i, t in enumerate(geo_df['country']):\n",
    "        t_cln = _clean(t)\n",
    "        t_pct_nm = t_cln + '_pct'\n",
    "        query_fmt += f'cast({t_cln} AS DOUBLE) / cast(total AS DOUBLE) \"{t_pct_nm}\"'\n",
    "        if i < len(geo_df['country']) - 1:\n",
    "            query_fmt += ',\\n'\n",
    "    \n",
    "    # applying above query\n",
    "    type_pct_df = sqldf(f'''\n",
    "    SELECT\n",
    "        a.*,\n",
    "        b.total,\n",
    "    {query_fmt}\n",
    "    FROM type_cnt_df a\n",
    "    JOIN tot_cnt_df b\n",
    "        ON a.source_singular = b.source_singular\n",
    "    ''', locals())\n",
    "    \n",
    "    # filter dataframe to get countries for each source that meet the criteria to be a tag\n",
    "    cntry_df = _generate_tag_df('country', geo_df, type_pct_df, total_min, total_max, pct_min)\n",
    "    \n",
    "    return cntry_df\n",
    "\n",
    "\n",
    "# takes extended DF as parameter\n",
    "def tag_time_period(df, n_clusters=4, init='random', n_init=10, max_iter=100, tol=1e-04, random_state=0):\n",
    "    # setting up K-Means clustering model to identify time periods to tag the data with\n",
    "    km = KMeans(\n",
    "        n_clusters=n_clusters, init=init,\n",
    "        n_init=n_init, max_iter=max_iter, \n",
    "        tol=tol, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # creating df that will be clustered by \"event_date\"\n",
    "    cluster_df = df\n",
    "    \n",
    "    # creating column transforming event_date to a unix timestamp for the clustering algorithm to work\n",
    "    cluster_df.event_date_unix = cluster_df['event_date'].apply(lambda x: pd.Timestamp(x).timestamp())\n",
    "\n",
    "    # reshaping to fit clustering algorithm requirements and fitting model\n",
    "    X = np.array(cluster_df.event_date_unix).reshape(-1, 1)\n",
    "    y_km = km.fit_predict(X)\n",
    "\n",
    "    # appending cluster labels back to df\n",
    "    cluster_df['time_period'] = y_km\n",
    "    \n",
    "    # selecting only required columns for returned df\n",
    "    cluster_df = cluster_df[['source_singular', 'time_period']]\n",
    "    \n",
    "    return cluster_df\n",
    "\n",
    "\n",
    "def tag_data(df):\n",
    "    base_df = extd_df[['source_singular']].drop_duplicates()\n",
    "    \n",
    "    for df in [tag_country(extd_df, 10, 1000, 0.75), tag_sub_event_type(extd_df, 10, 1000, 0.85), tag_time_period(extd_df)]:\n",
    "        base_df = sqldf('''\n",
    "            SELECT * FROM base_df a\n",
    "            JOIN tag_df b ON a.source_singular = b.source_singular\n",
    "        ''')\n",
    "\n",
    "    return base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "extd_df = pd.read_csv('../data/EXPANDED_acled_covid19.csv')\n",
    "\n",
    "# 10, 1000, 0.75\n",
    "base_df = extd_df[['source_singular']].drop_duplicates()\n",
    "# for df in [tag_country(extd_df, 10, 1000, 0.75), tag_sub_event_type(extd_df, 10, 1000, 0.85), tag_time_period(extd_df)]:\n",
    "for tag_df in [tag_country(extd_df, 10, 20, 0.75), tag_sub_event_type(extd_df, 10, 20, 0.85)]:\n",
    "    base_df = sqldf('''\n",
    "        SELECT * FROM base_df a\n",
    "        JOIN tag_df b ON a.source_singular = b.source_singular\n",
    "    ''')\n",
    "    \n",
    "# tag_country(extd_df, 10, 1000, 0.75)\n",
    "# tag_time_period(extd_df)\n",
    "# tag_sub_event_type(extd_df, 10, 1000, 0.85)\n",
    "base_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
